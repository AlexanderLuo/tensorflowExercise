{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "data_tuple = collections.namedtuple('data_tuple',['train','valid','test'])\n",
    "\n",
    "class DataSet():\n",
    "    #init function\n",
    "    def __init__(self,X):\n",
    "        self.X = X[:-1]\n",
    "        self.y = X[1:]\n",
    "    def \n",
    "\n",
    "    \n",
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "def _build_vocab(filename):\n",
    "    data = _read_words(filename)\n",
    "    counter = collections.Counter(data)\n",
    "    \n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "    data = _read_words(filename)\n",
    "    return [word_to_id[word] for word in data]\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "    \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "    Reads PTB text files, converts strings to integer ids,\n",
    "    and performs mini-batching of the inputs.\n",
    "    The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "    http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "    Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "      been extracted.\n",
    "    Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "    \"\"\"\n",
    "\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "    word_to_id = _build_vocab(train_path)\n",
    "    train_data = DataSet(_file_to_word_ids(train_path, word_to_id))\n",
    "    valid_data = DataSet(_file_to_word_ids(valid_path, word_to_id))\n",
    "    test_data = DataSet(_file_to_word_ids(test_path, word_to_id))\n",
    "    vocabulary = len(word_to_id)\n",
    "    return data_tuple(train_data,valid_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "    \n",
    "# def PTBReader():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path ='';\n",
    "\n",
    "os.path.join(data_path,\"train.txt\")\n",
    "\n",
    "os.path.join(data_path,\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named models.rnn.ptb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-951bad17b841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# flags = tf.flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named models.rnn.ptb"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "\n",
    "# flags = tf.flags\n",
    "# logging = tf.logging\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"model\", \"small\",\n",
    "#     \"A type of model. Possible options are: small, medium, large.\")\n",
    "# flags.DEFINE_string(\"data_path\", None, \"data_path\")\n",
    "# flags.DEFINE_bool(\"use_fp16\", False,\n",
    "#                   \"Train using 16-bit floats instead of 32bit floats\")\n",
    "\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "# def data_type():\n",
    "#   return tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "\n",
    "\n",
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, config):\n",
    "        \n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        # Slightly better results can be obtained with forget gate biases\n",
    "        # initialized to 1 but the hyperparameters of the model would need to be\n",
    "        # different than reported in the paper.\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_cell, output_keep_prob=config.keep_prob)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers, state_is_tuple=True)\n",
    "\n",
    "        self._initial_state = cell.zero_state(batch_size, data_type())\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\n",
    "                \"embedding\", [vocab_size, size], dtype=data_type())\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "        #input_size:(batch_size, num_steps,hidden_size)\n",
    "\n",
    "\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "        # Simplified version of tensorflow.models.rnn.rnn.py's rnn().\n",
    "        # This builds an unrolled LSTM for tutorial purposes only.\n",
    "        # In general, use the rnn() or state_saving_rnn() from rnn.py.\n",
    "        #\n",
    "        # The alternative version of the code below is:\n",
    "        #\n",
    "        # from tensorflow.models.rnn import rnn\n",
    "        # inputs = [tf.squeeze(input_, [1])\n",
    "        #           for input_ in tf.split(1, num_steps, inputs)]\n",
    "        # outputs, state = rnn.rnn(cell, inputs, initial_state=self._initial_state)\n",
    "        outputs = []\n",
    "        state = self._initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, size])\n",
    "        softmax_w = tf.get_variable(\n",
    "            \"softmax_w\", [size, vocab_size], dtype=data_type())\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=data_type())\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        loss = tf.nn.seq2seq.sequence_loss_by_example(\n",
    "            [logits],\n",
    "            [tf.reshape(self._targets, [-1])],\n",
    "            [tf.ones([batch_size * num_steps], dtype=data_type())])\n",
    "        self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "        self._final_state = state\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                          config.max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        self._new_lr = tf.placeholder(\n",
    "            tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "\n",
    "class SmallConfig(object):\n",
    "    \"\"\"Small config.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 13\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "\n",
    "\n",
    "class MediumConfig(object):\n",
    "    \"\"\"Medium config.\"\"\"\n",
    "    init_scale = 0.05\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 650\n",
    "    max_epoch = 6\n",
    "    max_max_epoch = 39\n",
    "    keep_prob = 0.5\n",
    "    lr_decay = 0.8\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "\n",
    "\n",
    "class LargeConfig(object):\n",
    "    \"\"\"Large config.\"\"\"\n",
    "    init_scale = 0.04\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 10\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 1500\n",
    "    max_epoch = 14\n",
    "    max_max_epoch = 55\n",
    "    keep_prob = 0.35\n",
    "    lr_decay = 1 / 1.15\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "\n",
    "\n",
    "class TestConfig(object):\n",
    "    \"\"\"Tiny config, for testing.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 1\n",
    "    num_layers = 1\n",
    "    num_steps = 2\n",
    "    hidden_size = 2\n",
    "    max_epoch = 1\n",
    "    max_max_epoch = 1\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "\n",
    "\n",
    "def run_epoch(session, model, data, eval_op, verbose=False):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    epoch_size = ((len(data) // model.batch_size) - 1) // model.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, model.batch_size,\n",
    "                                                    model.num_steps)):\n",
    "        fetches = [model.cost, model.final_state, eval_op]\n",
    "        feed_dict = {}\n",
    "        feed_dict[model.input_data] = x\n",
    "        feed_dict[model.targets] = y\n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        cost, state, _ = session.run(fetches, feed_dict)\n",
    "        costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "    if verbose and step % (epoch_size // 10) == 10:\n",
    "        print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "            (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "             iters * model.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    return np.exp(costs / iters)\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    if FLAGS.model == \"small\":\n",
    "        return SmallConfig()\n",
    "    elif FLAGS.model == \"medium\":\n",
    "        return MediumConfig()\n",
    "    elif FLAGS.model == \"large\":\n",
    "        return LargeConfig()\n",
    "    elif FLAGS.model == \"test\":\n",
    "        return TestConfig()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model: %s\", FLAGS.model)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    if not FLAGS.data_path:\n",
    "        raise ValueError(\"Must set --data_path to PTB data directory\")\n",
    "\n",
    "    raw_data = reader.ptb_raw_data(FLAGS.data_path)\n",
    "    train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "    config = get_config()\n",
    "    eval_config = get_config()\n",
    "    eval_config.batch_size = 1\n",
    "    eval_config.num_steps = 1\n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale,\n",
    "                                                    config.init_scale)\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(is_training=True, config=config)\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(is_training=False, config=config)\n",
    "        mtest = PTBModel(is_training=False, config=eval_config)\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for i in range(config.max_max_epoch):\n",
    "        lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n",
    "        m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "\n",
    "        print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        train_perplexity = run_epoch(session, m, train_data, m.train_op,\n",
    "                                   verbose=True)\n",
    "        print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        valid_perplexity = run_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "    test_perplexity = run_epoch(session, mtest, test_data, tf.no_op())\n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "3084 originated -> 12 as\n",
      "3084 originated -> 5239 anarchism\n",
      "12 as -> 3084 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 195 term\n",
      "6 a -> 12 as\n",
      "195 term -> 2 of\n",
      "195 term -> 6 a\n",
      "Initialized\n",
      "Average loss at step  0 :  270.847900391\n",
      "Nearest to between: prosecution, sedimentary, couturat, diddley, erudite, included, winch, misspelling,\n",
      "Nearest to known: ordination, thracian, bathing, creation, fretless, pessimism, rashid, monads,\n",
      "Nearest to history: darius, potawatomi, dietary, sins, dysprosium, dialectal, deletion, staples,\n",
      "Nearest to united: defences, iconography, dred, telegraph, eubacteria, haskell, luigi, gesture,\n",
      "Nearest to such: offspring, marcelo, mellaart, unseen, defraud, asian, universally, renewing,\n",
      "Nearest to many: epicycle, industrialists, levantine, emotions, roads, structure, genomic, hecate,\n",
      "Nearest to states: economist, dulles, participates, hler, ttingen, nasdaq, canmore, fincher,\n",
      "Nearest to was: tuna, involuntarily, cones, boitano, cabot, slumberland, ellipses, zoe,\n",
      "Nearest to that: restatement, concord, motile, rayner, spokesperson, commencing, bhp, unsophisticated,\n",
      "Nearest to seven: catwoman, clamps, realize, gradual, downfall, statement, shmuel, surinam,\n",
      "Nearest to about: cordwainer, delicacy, patrols, museu, hermon, engaged, coroner, patron,\n",
      "Nearest to nine: grind, variant, euphonium, lionheart, theodor, fiat, conciliatory, strafford,\n",
      "Nearest to over: carbonated, tokugawa, fateful, disperse, liverpool, rails, dolmens, dartmoor,\n",
      "Nearest to their: stateless, tetum, turtles, robotics, linger, hailstones, spp, jacobean,\n",
      "Nearest to UNK: eccles, modest, monograph, activates, matteo, khalifa, tr, crested,\n",
      "Nearest to it: erectile, verb, murchison, crewe, tracts, forums, subfield, becomes,\n",
      "Average loss at step  2000 :  113.902511683\n",
      "Average loss at step  4000 :  52.4075807097\n",
      "Average loss at step  6000 :  33.8466001366\n",
      "Average loss at step  8000 :  22.8361088331\n",
      "Average loss at step  10000 :  18.2041920273\n",
      "Nearest to between: prosecution, included, boards, vintage, ii, expulsions, treaty, in,\n",
      "Nearest to known: hold, quality, creation, kills, scrimmage, raions, mctaggart, doubleday,\n",
      "Nearest to history: darius, sunda, new, sins, punishment, carried, cable, abundant,\n",
      "Nearest to united: dred, archie, fermenting, others, size, technical, earlier, opponents,\n",
      "Nearest to such: asian, phi, noaa, well, pure, presumption, renewing, actor,\n",
      "Nearest to many: structure, roads, emotions, arslan, cinematography, levantine, autistic, tubing,\n",
      "Nearest to states: gb, archie, agave, economist, sulfuric, disagreement, yeast, te,\n",
      "Nearest to was: is, solf, leaders, has, manned, frau, mathbf, telephone,\n",
      "Nearest to that: and, archie, which, usually, motile, organs, mathbf, bckgr,\n",
      "Nearest to seven: nine, zero, eight, agave, aries, four, annexation, six,\n",
      "Nearest to about: heard, cordwainer, exclave, patron, tar, fraenkel, engaged, belgium,\n",
      "Nearest to nine: zero, eight, six, one, agave, mctaggart, alien, gland,\n",
      "Nearest to over: lymphoma, lewis, waldheim, austin, seized, affairs, just, alhazred,\n",
      "Nearest to their: the, stateless, large, solf, lymphoma, planned, experimentation, yeast,\n",
      "Nearest to UNK: alien, and, agave, one, the, archie, lymphoma, altenberg,\n",
      "Nearest to it: revival, a, mctaggart, likewise, solf, riots, bckgr, alien,\n",
      "Average loss at step  12000 :  13.8370418354\n",
      "Average loss at step  14000 :  11.9301634634\n",
      "Average loss at step  16000 :  10.1326428915\n",
      "Average loss at step  18000 :  8.52287695706\n",
      "Average loss at step  20000 :  7.92507608938\n",
      "Nearest to between: in, vintage, prosecution, included, ii, expulsions, circ, of,\n",
      "Nearest to known: bathing, ordination, hold, kills, barnes, scrimmage, quality, a,\n",
      "Nearest to history: darius, circ, dasyprocta, sins, booster, UNK, agouti, sunda,\n",
      "Nearest to united: circ, dred, dasyprocta, fermenting, archie, operatorname, agouti, fair,\n",
      "Nearest to such: well, and, being, ornaments, dasyprocta, implement, agha, noaa,\n",
      "Nearest to many: structure, roads, emotions, apologia, gum, truetype, levantine, cinematography,\n",
      "Nearest to states: agouti, gb, archie, dasyprocta, agave, theology, te, dulles,\n",
      "Nearest to was: is, has, by, had, are, were, as, in,\n",
      "Nearest to that: which, dasyprocta, circ, and, truetype, usually, operatorname, agouti,\n",
      "Nearest to seven: nine, eight, zero, six, four, agouti, dasyprocta, five,\n",
      "Nearest to about: delicacy, circ, incompressible, heard, exclave, patron, four, eight,\n",
      "Nearest to nine: eight, six, zero, five, seven, dasyprocta, circ, agouti,\n",
      "Nearest to over: nine, carbonated, lymphoma, affairs, resin, lewis, seized, waldheim,\n",
      "Nearest to their: the, his, dasyprocta, its, some, truetype, agouti, hides,\n",
      "Nearest to UNK: agouti, operatorname, circ, dasyprocta, peptide, truetype, alien, two,\n",
      "Nearest to it: he, this, the, agouti, circ, dasyprocta, which, operatorname,\n",
      "Average loss at step  22000 :  7.07362685287\n",
      "Average loss at step  24000 :  6.84028623676\n",
      "Average loss at step  26000 :  6.70760089004\n",
      "Average loss at step  28000 :  6.31658061087\n",
      "Average loss at step  30000 :  5.93033291459\n",
      "Nearest to between: in, with, vintage, included, from, expulsions, prosecution, circ,\n",
      "Nearest to known: ordination, bathing, chalukyas, barnes, olympus, hold, scrimmage, a,\n",
      "Nearest to history: darius, circ, dasyprocta, sins, booster, new, agouti, sunda,\n",
      "Nearest to united: circ, dred, fermenting, archie, dasyprocta, operatorname, fair, nunnery,\n",
      "Nearest to such: well, being, and, natal, implement, dasyprocta, ornaments, without,\n",
      "Nearest to many: structure, roads, emotions, apologia, asbestos, gum, truetype, levantine,\n",
      "Nearest to states: agouti, gb, agave, dulles, archie, yeast, te, dasyprocta,\n",
      "Nearest to was: is, were, had, has, by, are, be, seven,\n",
      "Nearest to that: which, usually, dasyprocta, circ, truetype, operatorname, agouti, also,\n",
      "Nearest to seven: eight, nine, four, six, five, zero, circ, three,\n",
      "Nearest to about: from, five, delicacy, exclave, seven, aba, heard, circ,\n",
      "Nearest to nine: eight, six, seven, five, four, zero, circ, dasyprocta,\n",
      "Nearest to over: carbonated, resin, seven, inch, affairs, seized, nine, waldheim,\n",
      "Nearest to their: the, his, its, some, dasyprocta, a, agouti, operatorname,\n",
      "Nearest to UNK: operatorname, circ, agouti, dasyprocta, peptide, truetype, six, aba,\n",
      "Nearest to it: he, this, which, there, agouti, circ, dasyprocta, operatorname,\n",
      "Average loss at step  32000 :  5.92210446608\n",
      "Average loss at step  34000 :  5.71250804484\n",
      "Average loss at step  36000 :  5.75912269568\n",
      "Average loss at step  38000 :  5.51418721092\n",
      "Average loss at step  40000 :  5.27763019657\n",
      "Nearest to between: in, with, from, vintage, included, expulsions, circ, marco,\n",
      "Nearest to known: bathing, ordination, barnes, chalukyas, udf, olympus, scrimmage, kills,\n",
      "Nearest to history: darius, circ, dasyprocta, sins, new, agouti, booster, ocular,\n",
      "Nearest to united: circ, dred, stitch, archie, fermenting, dasyprocta, operatorname, third,\n",
      "Nearest to such: well, being, natal, without, implement, dasyprocta, many, heracles,\n",
      "Nearest to many: structure, roads, several, some, emotions, apologia, these, asbestos,\n",
      "Nearest to states: agouti, gb, waveform, te, agave, sulfuric, dulles, archie,\n",
      "Nearest to was: is, were, has, had, by, are, be, became,\n",
      "Nearest to that: which, usually, but, dasyprocta, circ, this, it, truetype,\n",
      "Nearest to seven: six, eight, four, five, nine, three, zero, two,\n",
      "Nearest to about: from, four, vma, exclave, delicacy, aba, five, circ,\n",
      "Nearest to nine: eight, six, seven, zero, five, four, three, circ,\n",
      "Nearest to over: carbonated, resin, seven, waldheim, inch, fateful, nine, abelard,\n",
      "Nearest to their: his, its, the, some, dasyprocta, a, agouti, operatorname,\n",
      "Nearest to UNK: operatorname, circ, dasyprocta, agouti, peptide, kifl, four, vma,\n",
      "Nearest to it: he, this, which, there, circ, agouti, dasyprocta, they,\n",
      "Average loss at step  42000 :  5.40963388956\n",
      "Average loss at step  44000 :  5.24869075191\n",
      "Average loss at step  46000 :  5.28356995928\n",
      "Average loss at step  48000 :  5.23828103566\n",
      "Average loss at step  50000 :  4.99882346332\n",
      "Nearest to between: in, with, from, vintage, circ, expulsions, included, of,\n",
      "Nearest to known: barnes, udf, olympus, ordination, chalukyas, mede, bathing, heimdall,\n",
      "Nearest to history: circ, darius, sins, dasyprocta, mukherjee, new, agouti, ocular,\n",
      "Nearest to united: circ, dred, stitch, fermenting, archie, dasyprocta, telescopes, pitfalls,\n",
      "Nearest to such: well, natal, being, many, without, dasyprocta, implement, other,\n",
      "Nearest to many: some, several, these, roads, structure, emotions, apologia, all,\n",
      "Nearest to states: agouti, gb, solicitation, waveform, agave, sulfuric, dulles, dasyprocta,\n",
      "Nearest to was: is, were, has, had, by, be, agouti, aba,\n",
      "Nearest to that: which, however, this, usually, dasyprocta, but, bundestag, circ,\n",
      "Nearest to seven: six, eight, four, five, three, nine, zero, one,\n",
      "Nearest to about: from, roshan, five, vma, seven, exclave, heard, circ,\n",
      "Nearest to nine: eight, six, seven, zero, five, three, four, circ,\n",
      "Nearest to over: resin, carbonated, three, seven, mukherjee, fateful, inch, waldheim,\n",
      "Nearest to their: his, its, the, some, dasyprocta, her, agouti, operatorname,\n",
      "Nearest to UNK: dasyprocta, agouti, circ, operatorname, truetype, vma, peptide, lymphoma,\n",
      "Nearest to it: he, this, there, which, agouti, circ, dasyprocta, they,\n",
      "Average loss at step  52000 :  5.02176973605\n",
      "Average loss at step  54000 :  5.17420088613\n",
      "Average loss at step  56000 :  5.0145838691\n",
      "Average loss at step  58000 :  5.05190333426\n",
      "Average loss at step  60000 :  4.95864270014\n",
      "Nearest to between: with, in, from, circ, vintage, expulsions, albuquerque, smoking,\n",
      "Nearest to known: barnes, olympus, chalukyas, mede, udf, ordination, scrimmage, bathing,\n",
      "Nearest to history: circ, darius, sins, dasyprocta, mukherjee, ocular, agouti, new,\n",
      "Nearest to united: circ, dred, stitch, dasyprocta, archie, fermenting, michelob, telescopes,\n",
      "Nearest to such: well, natal, many, being, without, implement, dasyprocta, are,\n",
      "Nearest to many: some, several, these, other, roads, various, all, such,\n",
      "Nearest to states: agouti, solicitation, gb, microbats, waveform, agave, dasyprocta, dulles,\n",
      "Nearest to was: is, were, has, had, by, be, became, righteousness,\n",
      "Nearest to that: which, this, however, but, michelob, who, usually, dasyprocta,\n",
      "Nearest to seven: eight, six, five, nine, four, three, zero, dasyprocta,\n",
      "Nearest to about: five, roshan, michelob, from, vma, circ, six, cordwainer,\n",
      "Nearest to nine: eight, six, seven, four, five, zero, circ, dasyprocta,\n",
      "Nearest to over: resin, carbonated, inch, three, seven, mukherjee, fateful, aba,\n",
      "Nearest to their: his, its, the, some, her, dasyprocta, these, agouti,\n",
      "Nearest to UNK: operatorname, agouti, circ, dasyprocta, ursus, kapoor, vma, pulau,\n",
      "Nearest to it: he, this, there, which, they, circ, agouti, she,\n",
      "Average loss at step  62000 :  4.9890227567\n",
      "Average loss at step  64000 :  4.83562723637\n",
      "Average loss at step  66000 :  4.61316417068\n",
      "Average loss at step  68000 :  4.9607423948\n",
      "Average loss at step  70000 :  4.89598744059\n",
      "Nearest to between: with, in, from, vintage, expulsions, circ, through, albuquerque,\n",
      "Nearest to known: barnes, olympus, chalukyas, used, udf, defined, well, mede,\n",
      "Nearest to history: circ, darius, dasyprocta, mukherjee, sins, ocular, agouti, otimes,\n",
      "Nearest to united: circ, dred, fermenting, dasyprocta, archie, superscalar, stitch, telescopes,\n",
      "Nearest to such: well, many, natal, these, being, other, rupert, without,\n",
      "Nearest to many: some, several, these, all, other, various, such, roads,\n",
      "Nearest to states: upanija, agouti, solicitation, gb, waveform, microbats, agave, dulles,\n",
      "Nearest to was: is, were, has, had, became, by, righteousness, ursus,\n",
      "Nearest to that: which, however, this, bundestag, michelob, dasyprocta, usually, but,\n",
      "Nearest to seven: eight, six, nine, four, five, three, zero, operatorname,\n",
      "Nearest to about: roshan, michelob, from, four, five, microcebus, circ, exclave,\n",
      "Nearest to nine: eight, seven, six, five, four, zero, three, circ,\n",
      "Nearest to over: resin, carbonated, mico, mukherjee, inch, microcebus, fateful, three,\n",
      "Nearest to their: its, his, the, some, her, dasyprocta, these, lnot,\n",
      "Nearest to UNK: operatorname, circ, dasyprocta, ursus, agouti, peptide, microcebus, kapoor,\n",
      "Nearest to it: he, this, there, they, she, which, circ, agouti,\n",
      "Average loss at step  72000 :  4.74577236962\n",
      "Average loss at step  74000 :  4.8013269974\n",
      "Average loss at step  76000 :  4.72951307189\n",
      "Average loss at step  78000 :  4.8031367394\n",
      "Average loss at step  80000 :  4.80066793907\n",
      "Nearest to between: with, in, from, vintage, through, expulsions, circ, yolk,\n",
      "Nearest to known: used, defined, barnes, olympus, udf, chalukyas, well, such,\n",
      "Nearest to history: circ, darius, sins, mukherjee, dasyprocta, ocular, list, agouti,\n",
      "Nearest to united: circ, dred, fermenting, superscalar, patrolled, pitfalls, telescopes, archie,\n",
      "Nearest to such: well, these, many, natal, rupert, are, being, other,\n",
      "Nearest to many: some, several, these, all, various, other, such, roads,\n",
      "Nearest to states: upanija, agouti, solicitation, gb, agave, waveform, dulles, microbats,\n",
      "Nearest to was: is, were, has, had, became, by, agouti, be,\n",
      "Nearest to that: which, however, this, michelob, dasyprocta, but, usually, bundestag,\n",
      "Nearest to seven: six, eight, four, five, nine, three, one, zero,\n",
      "Nearest to about: michelob, roshan, from, microcebus, patron, cordwainer, exclave, circ,\n",
      "Nearest to nine: eight, seven, six, five, four, zero, circ, three,\n",
      "Nearest to over: resin, mico, microcebus, inch, carbonated, mukherjee, fateful, aba,\n",
      "Nearest to their: its, his, the, some, her, dasyprocta, these, agouti,\n",
      "Nearest to UNK: operatorname, agouti, dasyprocta, circ, ursus, microcebus, peptide, pulau,\n",
      "Nearest to it: he, this, there, she, which, they, circ, agouti,\n",
      "Average loss at step  82000 :  4.77910839653\n",
      "Average loss at step  84000 :  4.76349287045\n",
      "Average loss at step  86000 :  4.77621400201\n",
      "Average loss at step  88000 :  4.7594513731\n",
      "Average loss at step  90000 :  4.7275628953\n",
      "Nearest to between: with, in, from, within, vintage, through, expulsions, circ,\n",
      "Nearest to known: used, well, such, defined, udf, carotenoids, chalukyas, olympus,\n",
      "Nearest to history: circ, darius, ocular, mukherjee, dasyprocta, sins, list, agouti,\n",
      "Nearest to united: circ, dred, superscalar, fermenting, archie, dasyprocta, pitfalls, telescopes,\n",
      "Nearest to such: well, these, many, natal, rupert, being, known, without,\n",
      "Nearest to many: some, several, these, various, all, other, such, peacocks,\n",
      "Nearest to states: upanija, agouti, solicitation, gb, waveform, agave, antwerp, lit,\n",
      "Nearest to was: is, were, had, has, became, been, by, be,\n",
      "Nearest to that: which, however, this, but, usually, michelob, dasyprocta, who,\n",
      "Nearest to seven: eight, five, six, four, nine, three, zero, one,\n",
      "Nearest to about: cordwainer, michelob, roshan, microcebus, from, exclave, patron, circ,\n",
      "Nearest to nine: eight, seven, six, five, four, zero, three, circ,\n",
      "Nearest to over: resin, carbonated, inch, microcebus, mico, mukherjee, three, aba,\n",
      "Nearest to their: its, his, the, her, some, dasyprocta, these, agouti,\n",
      "Nearest to UNK: operatorname, agouti, dasyprocta, circ, microcebus, callithrix, ursus, pulau,\n",
      "Nearest to it: he, this, there, which, she, they, but, circ,\n",
      "Average loss at step  92000 :  4.67206735861\n",
      "Average loss at step  94000 :  4.72062826681\n",
      "Average loss at step  96000 :  4.68714044118\n",
      "Average loss at step  98000 :  4.60340626132\n",
      "Average loss at step  100000 :  4.70759905589\n",
      "Nearest to between: with, in, from, within, through, vintage, constituci, expulsions,\n",
      "Nearest to known: used, such, well, olympus, defined, udf, carotenoids, chalukyas,\n",
      "Nearest to history: circ, ocular, darius, mukherjee, dasyprocta, list, otimes, agouti,\n",
      "Nearest to united: circ, dred, superscalar, fermenting, pitfalls, telescopes, patrolled, ventura,\n",
      "Nearest to such: well, these, many, natal, known, rupert, being, some,\n",
      "Nearest to many: some, several, these, all, various, such, boutros, other,\n",
      "Nearest to states: upanija, agouti, solicitation, gb, agave, waveform, lit, nations,\n",
      "Nearest to was: is, were, has, had, became, been, be, by,\n",
      "Nearest to that: which, however, this, but, usually, michelob, dasyprocta, milne,\n",
      "Nearest to seven: eight, six, four, five, nine, three, zero, circ,\n",
      "Nearest to about: cordwainer, michelob, roshan, from, microcebus, patron, exclave, over,\n",
      "Nearest to nine: eight, seven, six, five, zero, four, three, circ,\n",
      "Nearest to over: resin, carbonated, inch, three, microcebus, mico, mukherjee, seven,\n",
      "Nearest to their: its, his, her, the, some, these, dasyprocta, our,\n",
      "Nearest to UNK: operatorname, agouti, ursus, kapoor, dasyprocta, circ, iit, peptide,\n",
      "Nearest to it: he, this, there, she, they, which, circ, agouti,\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Basic word2vec example.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Download the data.\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "        statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "vocabulary = read_data(filename)\n",
    "print('Data size', len(vocabulary))\n",
    "\n",
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 50000\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                            vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "        while target in targets_to_avoid:\n",
    "            target = random.randint(0, span - 1)\n",
    "        targets_to_avoid.append(target)\n",
    "        batch[i * num_skips + j] = buffer[skip_window]\n",
    "        labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "        data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]],\n",
    "        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "        if step > 0:\n",
    "            average_loss /= 2000\n",
    "        # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "        print('Average loss at step ', step, ': ', average_loss)\n",
    "        average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "        sim = similarity.eval()\n",
    "        for i in xrange(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in xrange(top_k):\n",
    "            close_word = reverse_dictionary[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "except ImportError:\n",
    "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-1-f8a30e79334f>\", line 5, in <module>\n    \"embedding\", [100, 10], dtype=np.int32)\n  File \"/home/lrh/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/lrh/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f8a30e79334f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m embedding = tf.get_variable(\n\u001b[0;32m----> 5\u001b[0;31m             \"embedding\", [100, 10], dtype=np.int32)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#inputs = tf.nn.embedding_lookup(embedding, self._input_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lrh/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/home/lrh/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/home/lrh/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/home/lrh/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lrh/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-1-f8a30e79334f>\", line 5, in <module>\n    \"embedding\", [100, 10], dtype=np.int32)\n  File \"/home/lrh/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/lrh/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "embedding = tf.get_variable(\n",
    "            \"embedding\", [100, 10], dtype=np.int32)\n",
    "#inputs = tf.nn.embedding_lookup(embedding, self._input_data)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
